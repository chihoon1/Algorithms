{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c1daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pi, sqrt, exp, log2, ceil\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e69acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTreeNode():\n",
    "    def __init__(self, instance):\n",
    "        self.node = instance\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (str(self.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91a59e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTree():\n",
    "    def __init__(self, root=None):\n",
    "        root_node = BTreeNode(root) if root is not None else None\n",
    "        self.root = root_node\n",
    "        # self.size = 1\n",
    "\n",
    "\n",
    "    def BFS_traverse(self):\n",
    "        # print out Binary Tree in BFS traversal order\n",
    "        unvisited = Queue()\n",
    "        unvisited.put(self.root)\n",
    "        nodes_in_the_same_level = []\n",
    "\n",
    "        while not unvisited.empty():\n",
    "            curr_node = unvisited.get()\n",
    "            nodes_in_the_same_level.append([curr_node])\n",
    "            if curr_node is not None:\n",
    "                unvisited.put(curr_node.left_child)\n",
    "                unvisited.put(curr_node.right_child)\n",
    "        count = 0\n",
    "        exp_of_two = 1\n",
    "        _ = []\n",
    "        for i in range(len(nodes_in_the_same_level)):\n",
    "            count += 1\n",
    "            _.append(nodes_in_the_same_level[i])\n",
    "            if count == exp_of_two:\n",
    "                print(f\"level {log2(exp_of_two)}\")\n",
    "                for j in range(len(_)):\n",
    "                    yes_or_no = \"Yes\" if j % 2 == 0 else \"No\"\n",
    "                    print(f\"{yes_or_no}: {_[j]}\")\n",
    "                count = 0\n",
    "                exp_of_two *= 2\n",
    "                _ = []\n",
    "        print(f\"level {ceil(log2(exp_of_two))}\")\n",
    "        for j in range(len(_)):\n",
    "            yes_or_no = \"Yes\" if j % 2 == 0 else \"No\"\n",
    "            print(f\"{yes_or_no}: {_[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c2b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, D, class_labels, is_categorical=False):\n",
    "        # param: D(pd.DataFrame) is a data set. class_labels(pd.DataFrame) is a class label(y) for each point xj in D\n",
    "        # param: is_categorical(Bool) true if D based on categorical attributes\n",
    "        # Merge D and class_labels. The last column of data_set will contain the class labels for each data point\n",
    "        if D.shape[0] != class_labels.shape[0]: raise IndexError(\"Data and Class Label must have the same number of rows\")\n",
    "        data_set = D.copy()\n",
    "        #print(class_labels.iloc[:,0])\n",
    "        data_set.insert(loc=len(data_set.columns), column='classes', value=class_labels.iloc[:,0])\n",
    "        self.data_set = data_set  # class labels of each data point inserted in the last column\n",
    "        self.is_categorical = is_categorical\n",
    "        self.class_domain = list(set(class_labels.iloc[:, 0]))\n",
    "        self.decision_tree = BinaryTree()\n",
    "        self.is_trained = False\n",
    "\n",
    "\n",
    "    def calculate_entropy(self, nvi_lst):\n",
    "        #param: nvi_lst(list) # of counts of attribute Xj <=v(split point) and xj=Class i in the data set\n",
    "        try:\n",
    "            total_pts_counts = sum(nvi_lst)  # total number of data points in the Data\n",
    "            entropy = 0\n",
    "            for i in range(len(nvi_lst)):\n",
    "                prob_class_i_given_D = nvi_lst[i] / total_pts_counts\n",
    "                entropy += prob_class_i_given_D * log2(prob_class_i_given_D)\n",
    "        except ZeroDivisionError:\n",
    "            # this happens when computing the entropy of data set with no data point\n",
    "            # In this case, I assumed the entropy would be zero as it shouldn't affect the result of the algorithm\n",
    "            entropy = 0\n",
    "        return entropy * -1\n",
    "\n",
    "\n",
    "    def calc_info_gain(self, nvi_lst, nci_lst):\n",
    "        # param: nvi_lst(list) # of counts of attribute Xj <=v(split point) and xj=Class i in the data set(or splitted)\n",
    "        # param:nci_lst(list) # of counts of xj=Class i in the data set(can be splitted data set)\n",
    "        D_pts_counts = sum(nci_lst)\n",
    "        Y_pts_counts = sum(nvi_lst)\n",
    "        N_pts_counts = D_pts_counts - Y_pts_counts\n",
    "        entroy_D = self.calculate_entropy(nci_lst)\n",
    "        entroy_D_Y = self.calculate_entropy(nvi_lst)\n",
    "        nvi_complement_lst = [nci_lst[i] - nvi_lst[i] for i in range(len(nci_lst))]\n",
    "        entroy_D_N = self.calculate_entropy(nvi_complement_lst)\n",
    "        info_gain = entroy_D - (Y_pts_counts/D_pts_counts*entroy_D_Y) - (N_pts_counts/D_pts_counts*entroy_D_N)\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def search_optimal_numeric_split_point(self, attr_index):\n",
    "        # param: attr_index(int) indicates the column index of attribute where we want to find the best split point\n",
    "        modified_D = self.data_set.loc[:, [self.data_set.columns[attr_index], 'classes']]\n",
    "        Xj_attribute_class_lst = [[modified_D.iloc[i,0],modified_D.iloc[i,1]] for i in range(len(modified_D))]\n",
    "        Xj_attribute_class_lst.sort(key=lambda x: x[0])  # 2d list. first column: attribute, second column: class\n",
    "        midp_lst = []\n",
    "        all_n_v_class_i_lst = []  # list of lists containing nvi for all midpoints v\n",
    "        n_class_i_lst = [0] * len(self.class_domain)  # list containing the counts of data points for all cluster in data set\n",
    "        for k in range(len(Xj_attribute_class_lst)):  # loop through all data points except for the last data point\n",
    "            if k < len(Xj_attribute_class_lst) - 1:\n",
    "                midpoint = (Xj_attribute_class_lst[k][0] + Xj_attribute_class_lst[k+1][0])/2\n",
    "            n_class_i_lst[self.class_domain.index(Xj_attribute_class_lst[k][1])] += 1\n",
    "            if k > 0 and Xj_attribute_class_lst[k][0] <= midp_lst[-1]:  # this happens when two same values of attribute Xj exist\n",
    "                all_n_v_class_i_lst[-1] = n_class_i_lst[:]\n",
    "            if k == 0 or midp_lst[-1] != midpoint:  # midpoint must be unique\n",
    "                midp_lst.append(midpoint)\n",
    "                all_n_v_class_i_lst.append(n_class_i_lst[:])\n",
    "        optimal_split_point, best_score = None, -float('inf')\n",
    "        for k in range(len(midp_lst)):\n",
    "            score = self.calc_info_gain(all_n_v_class_i_lst[k], n_class_i_lst)\n",
    "            print(f\"{attr_index}-th column split point {midp_lst[k]} information gain: {score}\")\n",
    "            if best_score < score:\n",
    "                optimal_split_point = midp_lst[k]\n",
    "                best_score = score\n",
    "        return optimal_split_point, best_score\n",
    "\n",
    "\n",
    "    def search_optimal_categorical_split_point(self, attr_index):\n",
    "        # param: attr_index(int) indicates the column index of attribute where we want to find the best split point\n",
    "        modified_D = self.data_set.loc[:, [self.data_set.columns[attr_index], 'classes']]\n",
    "        Xj_attribute_class_lst = [[modified_D.iloc[i, 0], modified_D.iloc[i, 1]] for i in range(len(modified_D))]\n",
    "        n_class_i_lst = [0] * len(self.class_domain)  # contains the counts of data points for all cluster in data set\n",
    "        all_n_v_class_i_dict = {}  # dictionary containing as value nvi for all categories vi in dom(Xj), (key: vi)\n",
    "        for i in range(len(Xj_attribute_class_lst)):\n",
    "            n_class_i_lst[self.class_domain.index(Xj_attribute_class_lst[i][1])] += 1\n",
    "            if all_n_v_class_i_dict.get(Xj_attribute_class_lst[i][0]) is None:\n",
    "                all_n_v_class_i_dict[Xj_attribute_class_lst[i][0]] = [0]*len(self.class_domain)\n",
    "        for i in range(len(Xj_attribute_class_lst)):\n",
    "            class_index = self.class_domain.index(Xj_attribute_class_lst[i][1])\n",
    "            category_attr_val = Xj_attribute_class_lst[i][0]\n",
    "            all_n_v_class_i_dict[category_attr_val][class_index] += 1\n",
    "        optimal_split_point, best_score = None, -float('inf')\n",
    "        for key, val in all_n_v_class_i_dict.items():\n",
    "            score = self.calc_info_gain(val, n_class_i_lst)\n",
    "            print(f\"{attr_index}-th column split point {key} information gain: {score}\")\n",
    "            if best_score < score:\n",
    "                optimal_split_point = key\n",
    "                best_score = score\n",
    "        return optimal_split_point, best_score\n",
    "\n",
    "\n",
    "    def paritioning_regions(self, D, purity_threshold, num_pts_threshold, parent_split_pt=None):\n",
    "        # param: D is data set(or Region/splitted data set)\n",
    "        # param: purity_threshold indicates that if max purity of D <= this threshold, then leaf node created\n",
    "        # param: num_pts_threshold(int, default=0) indicates that if |D| is >= this threshold, then leaf node created\n",
    "        # param: parent_split_pt(tuple) is a split point and its attribute column index of parent region\n",
    "        # return list containing split points in the order of Depth-First\n",
    "        print(f\"Parent split point: {parent_split_pt}\")\n",
    "        n = len(D)\n",
    "        n_class_i_lst = [0] * len(self.class_domain)\n",
    "        for i in range(len(D)):\n",
    "            n_class_i_lst[self.class_domain.index(D.iloc[i, -1])] += 1\n",
    "        max_purity, c_star = -1, None  # c_star is the class with the max purity\n",
    "        for i in range(len(n_class_i_lst)):\n",
    "            if max_purity < n_class_i_lst[i]:\n",
    "                max_purity = n_class_i_lst[i]\n",
    "                c_star = self.class_domain[i]\n",
    "        if n <= num_pts_threshold or max_purity / n >= purity_threshold:\n",
    "            return BTreeNode((D, c_star))\n",
    "        best_split_pt, max_score = None, -float('inf')\n",
    "        for col_j in range(D.shape[1] - 1):\n",
    "            if self.is_categorical:\n",
    "                split_pt, score = self.search_optimal_categorical_split_point(col_j)\n",
    "            else:\n",
    "                split_pt, score = self.search_optimal_numeric_split_point(col_j)\n",
    "            if max_score < score and ((split_pt, col_j) != parent_split_pt):\n",
    "                best_split_pt = (split_pt, col_j)\n",
    "                max_score = score\n",
    "        quot = \"'\" if type(best_split_pt[0]) == str else \"\"\n",
    "        yes_expr = \"==\" if self.is_categorical else \"<=\"\n",
    "        no_expr = \"!=\" if self.is_categorical else \">\"\n",
    "        query1 = f\"{D.columns[best_split_pt[1]]} {yes_expr} {quot}{best_split_pt[0]}{quot}\"\n",
    "        query2 = f\"{D.columns[best_split_pt[1]]} {no_expr} {quot}{best_split_pt[0]}{quot}\"\n",
    "        D_Y = D.query(query1)\n",
    "        D_N = D.query(query2)\n",
    "        node = BTreeNode(best_split_pt)\n",
    "        if self.decision_tree.root is None:\n",
    "            self.decision_tree.root = node\n",
    "        node.left_child = self.paritioning_regions(D_Y, purity_threshold, num_pts_threshold, best_split_pt)\n",
    "        node.right_child = self.paritioning_regions(D_N, purity_threshold, num_pts_threshold, best_split_pt)\n",
    "        return node\n",
    "\n",
    "\n",
    "    def train(self, purity_threshold, num_pts_threshold=0):\n",
    "        # param: purity_threshold indicates that if max purity of D <= this threshold, then leaf node created\n",
    "        # param: num_pts_threshold(int, default=0) indicates that if |D| is >= this threshold, then leaf node created\n",
    "        # return list containing split points in the order of Depth-First\n",
    "        self.paritioning_regions(self.data_set, purity_threshold, num_pts_threshold)\n",
    "        self.is_trained = True\n",
    "        return self.decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a824111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent split point: None\n",
      "0-th column split point 10 information gain: 0.003228943620363578\n",
      "0-th column split point 20 information gain: 0.0032289436203635224\n",
      "1-th column split point A information gain: 0.15886800584993\n",
      "1-th column split point B information gain: 0.15886800584993005\n",
      "Parent split point: ('B', 1)\n",
      "Parent split point: ('B', 1)\n",
      "0-th column split point 10 information gain: 0.003228943620363578\n",
      "0-th column split point 20 information gain: 0.0032289436203635224\n",
      "1-th column split point A information gain: 0.15886800584993\n",
      "1-th column split point B information gain: 0.15886800584993005\n",
      "Parent split point: (10, 0)\n",
      "Parent split point: (10, 0)\n",
      "level 0.0\n",
      "Yes: [('B', 1)]\n",
      "level 1.0\n",
      "Yes: [(   Price Chef classes\n",
      "3     10    B       H\n",
      "4     10    B       H\n",
      "5     10    B       H\n",
      "6     20    B       L\n",
      "7     20    B       H, 'H')]\n",
      "No: [(10, 0)]\n",
      "level 2.0\n",
      "Yes: [None]\n",
      "No: [None]\n",
      "Yes: [(   Price Chef classes\n",
      "0     10    A       L\n",
      "1     10    A       L, 'L')]\n",
      "No: [(   Price Chef classes\n",
      "2     20    A       H, 'H')]\n",
      "level 3\n",
      "Yes: [None]\n",
      "No: [None]\n",
      "Yes: [None]\n",
      "No: [None]\n"
     ]
    }
   ],
   "source": [
    "price = [10, 10, 20, 10, 10, 10, 20, 20]\n",
    "chef = ['A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']\n",
    "class_quality = ['L', 'L', 'H', 'H', 'H', 'H', 'L', 'H']\n",
    "D = pd.DataFrame({'Price': price, 'Chef': chef})\n",
    "class_labels = pd.DataFrame({'Quality': class_quality})\n",
    "\n",
    "DTC = DecisionTreeClassifier(D, class_labels, is_categorical=True)\n",
    "DTC.train(purity_threshold=0.75)\n",
    "if DTC.is_trained:\n",
    "    DTC.decision_tree.BFS_traverse()\n",
    "else:\n",
    "    print(\"Not Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3909e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c2fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
